{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "task_files = 'data/bbh/cot-prompts-1-shot'\n",
    "task_files = glob.glob(task_files + '/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma-2b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of domain tasks:\n",
      "['data/bbh/cot-prompts-1-shot/dyck_languages.txt', 'data/bbh/cot-prompts-1-shot/geometric_shapes.txt', 'data/bbh/cot-prompts-1-shot/temporal_sequences.txt', 'data/bbh/cot-prompts-1-shot/web_of_lies.txt', 'data/bbh/cot-prompts-1-shot/snarks.txt', 'data/bbh/cot-prompts-1-shot/multistep_arithmetic_two.txt', 'data/bbh/cot-prompts-1-shot/boolean_expressions.txt', 'data/bbh/cot-prompts-1-shot/penguins_in_a_table.txt', 'data/bbh/cot-prompts-1-shot/logical_deduction_three_objects.txt']\n"
     ]
    }
   ],
   "source": [
    "# random split into two sets\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(task_files)\n",
    "iid_tasks = task_files[:18]\n",
    "ood_tasks = task_files[18:]\n",
    "print(\"Out of domain tasks:\")\n",
    "print(ood_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cot prompt of each task\n",
    "cot_prompts = {\n",
    "    task_name: open(task_name).read()\n",
    "    for task_name in task_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2task = {\n",
    "    prompt:task.split('/')[-1].split('.')[0]\n",
    "    for task, prompt in cot_prompts.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get iid prompts\n",
    "iid_prompts = set([cot_prompts[task_name] for task_name in iid_tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_results_path = f'data/bbh/results/{model_name}/text_generations_bbh.json'\n",
    "import json\n",
    "text_generation_results = json.load(open(text_generation_results_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4687 iid generations and 1824 ood generations\n"
     ]
    }
   ],
   "source": [
    "# find the index of iid generation\n",
    "# \"question\" in iid generation should be start with any of the iid prompts\n",
    "iid_index = []\n",
    "ood_index = []\n",
    "for i, gen in enumerate(text_generation_results):\n",
    "    for prompt in cot_prompts.values():\n",
    "        if gen['question'].startswith(prompt):\n",
    "            if 'task' not in text_generation_results[i]:\n",
    "                text_generation_results[i]['task'] = prompt2task[prompt]\n",
    "            if prompt in iid_prompts:\n",
    "                iid_index.append(i)\n",
    "            else:\n",
    "                ood_index.append(i)\n",
    "            \n",
    "assert len(iid_index) + len(ood_index) == len(text_generation_results)\n",
    "\n",
    "print(f\"There are {len(iid_index)} iid generations and {len(ood_index)} ood generations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after added the task type, save the generation results\n",
    "with open(text_generation_results_path, 'w') as f:\n",
    "    json.dump(text_generation_results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/bbh/results/{model_name}/text_generations_bbh.json\n",
    "# data/bbh/results/{model_name}/text_generations_bbh-3-shot.json\n",
    "# data/bbh/results/{model_name}/hidden_states_bbh.pt\n",
    "# data/bbh/results/{model_name}/hidden_states_bbh-3-shot.pt\n",
    "# for the above files, we need to split the results into iid and ood\n",
    "# then move to the corresponding folder, like\n",
    "# data/bbh-iid/results/{model_name}/text_generations_bbh.json\n",
    "# data/bbh-ood/results/{model_name}/text_generations_bbh-3-shot.json\n",
    "# data/bbh-iid/results/{model_name}/hidden_states_bbh.pt\n",
    "# data/bbh-ood/results/{model_name}/hidden_states_bbh-3-shot.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please add the task type to the generation results first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(ood_file), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m iid_results[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease add the task type to the generation results first\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ood_results[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease add the task type to the generation results first\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(iid_results, \u001b[38;5;28mopen\u001b[39m(iid_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(ood_results, \u001b[38;5;28mopen\u001b[39m(ood_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please add the task type to the generation results first"
     ]
    }
   ],
   "source": [
    "# for text_generations\n",
    "for file in [\n",
    "    f'data/bbh/results/{model_name}/text_generations_bbh.json',\n",
    "    # f'data/bbh/results/{model_name}/text_generations_bbh-3-shot.json',\n",
    "]:\n",
    "    results = json.load(open(file))\n",
    "    \n",
    "    assert 'task' in results[0], \"Please add the task type to the generation results first\"\n",
    "    \n",
    "    iid_results = [results[i] for i in iid_index]\n",
    "    ood_results = [results[i] for i in ood_index]\n",
    "    iid_file = file.replace('/bbh/', '/bbh-iid/')\n",
    "    ood_file = file.replace('/bbh/', '/bbh-ood/')\n",
    "    \n",
    "    # make the folder if not exist\n",
    "    os.makedirs(os.path.dirname(iid_file), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(ood_file), exist_ok=True)\n",
    "    \n",
    "    assert 'task' in iid_results[0], 'Please add the task type to the generation results first'\n",
    "    assert 'task' in ood_results[0], 'Please add the task type to the generation results first'\n",
    "    \n",
    "    json.dump(iid_results, open(iid_file, 'w'), indent=2)\n",
    "    json.dump(ood_results, open(ood_file, 'w'), indent=2)\n",
    "    \n",
    "# for hidden_states\n",
    "for file in [\n",
    "    f'data/bbh/results/{model_name}/hidden_states_bbh.pt',\n",
    "    # f'data/bbh/results/{model_name}/hidden_states_bbh-3-shot.pt',\n",
    "]:\n",
    "    results = torch.load(file)\n",
    "    iid_results = results[iid_index]\n",
    "    ood_results = results[ood_index]\n",
    "    iid_file = file.replace('/bbh/', '/bbh-iid/')\n",
    "    ood_file = file.replace('/bbh/', '/bbh-ood/')\n",
    "    \n",
    "    # make the folder if not exist\n",
    "    os.makedirs(os.path.dirname(iid_file), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(ood_file), exist_ok=True)\n",
    "    \n",
    "    torch.save(iid_results, iid_file)\n",
    "    torch.save(ood_results, ood_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch212",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
