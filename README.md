# Evaluating P(IK)

This repository contains the code for experiments aiming to evaluate P(IK) in large language models (probability of "I Know") to assess their calibration (i.e. correctly stating how uncertain they are about their answers).

This builds up on Anthropic's work ['Language Models (Mostly) Know What They Know'](https://arxiv.org/abs/2106.03384), using probing techniques like in ['Discovering Latent Knowledge in Language Models Without Supervision'](https://arxiv.org/abs/2212.03827), where a linear probe $p(IK) = \sigma(v^T h)$ is trained on the hidden activations of the model rather than the model's output.

## Usage

The following scripts are provided:

- `inference.py`: inference example, provides completion for a (zero-shot) prompt.
- `evaluate.py`: evaluates the model on a (portion of) the validation set of the [TriviaQA dataset](https://huggingface.co/datasets/trivia_qa). Multiple answers can be sampled for each question.
- `generate.py`: given a model and its answers to the TriviaQA questions, saves the hidden activations of the model for each question-answer pair.
- `train.py`: trains a linear probe on the hidden activations generated by `generate.py` to evaluate the model's calibration.

The following are some of the arguments that can be passed to the scripts:

- `n_questions`: number of q-a pairs to generate/evaluate.
- `model_checkpoint`: model checkpoint to use. The model must be a [transformers](https://huggingface.co/transformers/) model.
- `precision`: model precision (float16 or float32).
- `n_answers_per_question`: number of answers to generate per question.
- `max_new_tokens`: maximum number of tokens to generate per answer.
- `temperature`: temperature for generation.
- `num_epochs`: number of epochs to train the linear probe for.
- `batch_size`: batch size for training the linear probe.
- `learning_rate`: learning rate for training the linear probe.

For larger models like GPT-J, it could be useful to download a sharded version of the model. Specifically, for the `EleutherAI/gpt-j-6B` checkpoint, the class `Model` looks for a model in the `sharded-gpt-j-6B` folder, that can be downloaded with the following commands:

```bash
git clone https://huggingface.co/sgugger/sharded-gpt-j-6B
cd sharded-gpt-j-6B && git-lfs install && git pull
```

The `data` folder is used to store the hidden states generated by `generate.py` to train the linear probe. The following files are generated:

- `hidden_states.pt`: hidden states of the model for each question-answer pair.
- `qa_pairs.csv`: question-answer pairs.
- `text_generations.csv`: text generations for each question-answer pair (multiple answers can be generated for each question).
