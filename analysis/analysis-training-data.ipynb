{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "# data/bbh-full\n",
    "# find all json files in the directory\n",
    "files = glob.glob('data/bbh-full/*.json')\n",
    "\n",
    "from collections import Counter\n",
    "full_task_examples_count = Counter()\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        task = file.split('/')[-1].split('.')[0]\n",
    "        data = json.load(f)\n",
    "        full_task_examples_count[task] = len(data['examples'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hyperbaton': 50000,\n",
       "         'formal_fallacies': 14200,\n",
       "         'task': 14200,\n",
       "         'reasoning_about_colored_objects': 2000,\n",
       "         'word_sorting': 1900,\n",
       "         'tracking_shuffled_objects_seven_objects': 1750,\n",
       "         'tracking_shuffled_objects_five_objects': 1250,\n",
       "         'object_counting': 1000,\n",
       "         'dyck_languages': 1000,\n",
       "         'temporal_sequences': 1000,\n",
       "         'sports_understanding': 1000,\n",
       "         'navigate': 1000,\n",
       "         'salient_translation_error_detection': 998,\n",
       "         'tracking_shuffled_objects_three_objects': 750,\n",
       "         'logical_deduction_seven_objects': 700,\n",
       "         'movie_recommendation': 500,\n",
       "         'logical_deduction_five_objects': 500,\n",
       "         'ruin_names': 448,\n",
       "         'date_understanding': 369,\n",
       "         'geometric_shapes': 360,\n",
       "         'logical_deduction_three_objects': 300,\n",
       "         'disambiguation_qa': 258,\n",
       "         'causal_judgement': 190,\n",
       "         'snarks': 181,\n",
       "         'penguins_in_a_table': 149})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_task_examples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh_task_examples_count = Counter()\n",
    "for file in glob.glob('data/bbh/bbh/*.json'):\n",
    "    with open(file) as f:\n",
    "        task = file.split('/')[-1].split('.')[0]\n",
    "        data = json.load(f)\n",
    "        bbh_task_examples_count[task] = len(data['examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'object_counting': 250,\n",
       "         'movie_recommendation': 250,\n",
       "         'multistep_arithmetic_two': 250,\n",
       "         'boolean_expressions': 250,\n",
       "         'date_understanding': 250,\n",
       "         'dyck_languages': 250,\n",
       "         'tracking_shuffled_objects_three_objects': 250,\n",
       "         'formal_fallacies': 250,\n",
       "         'word_sorting': 250,\n",
       "         'logical_deduction_three_objects': 250,\n",
       "         'temporal_sequences': 250,\n",
       "         'logical_deduction_seven_objects': 250,\n",
       "         'tracking_shuffled_objects_seven_objects': 250,\n",
       "         'logical_deduction_five_objects': 250,\n",
       "         'salient_translation_error_detection': 250,\n",
       "         'ruin_names': 250,\n",
       "         'sports_understanding': 250,\n",
       "         'reasoning_about_colored_objects': 250,\n",
       "         'geometric_shapes': 250,\n",
       "         'tracking_shuffled_objects_five_objects': 250,\n",
       "         'navigate': 250,\n",
       "         'web_of_lies': 250,\n",
       "         'disambiguation_qa': 250,\n",
       "         'hyperbaton': 250,\n",
       "         'causal_judgement': 187,\n",
       "         'snarks': 178,\n",
       "         'penguins_in_a_table': 146})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbh_task_examples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snarks 181 178\n",
      "penguins_in_a_table 149 146\n",
      "logical_deduction_three_objects 300 250\n",
      "causal_judgement 190 187\n",
      "disambiguation_qa 258 250\n"
     ]
    }
   ],
   "source": [
    "# find the task that full has more examples than bbh\n",
    "for task, count in full_task_examples_count.items():\n",
    "    if count - bbh_task_examples_count[task] < 100:\n",
    "        print(task, count, bbh_task_examples_count[task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multistep_arithmetic_two 250\n",
      "boolean_expressions 250\n",
      "web_of_lies 250\n"
     ]
    }
   ],
   "source": [
    "# FIND THE TASK THAT not in full but in bbh\n",
    "for task, count in bbh_task_examples_count.items():\n",
    "    if task not in full_task_examples_count:\n",
    "        print(task, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch212",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
